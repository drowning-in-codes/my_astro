---
title: '深度学习问题'
description: '深度学习问题'
pubDate: 'Oct 7 2023'
heroImage: '/placeholder-hero.jpg'
---

## 1.验证z = sum(wx)+b标准差为sqrt(3/2)，假设Var(w)=1/1000, n=500。

由于w,x,b变量相互独立, 根据方差的计算以及权重的初始化方式,由于已经假设输入数据符合N~(0,1),所以sum(wx)的方差是500/1000=0.5,

var(z) = 0.5+1

所以z的标准差是√3/2

> 提示：权重初始化目的是为了让每一层的输出z(神经元前的那一项)和该层输入 x 保持一致的数据分布。权重初始化的方法是对权重的数据除以输入权重神经元个数的根号，注意 w 和 x 和 b 三个变量相互独立。

## 2.什么是训练集、验证集、测试集？在机器学习中为什么需要将样本分成独立的三部分：训练集（train set），验证集（validation set ) 和测试集（test set）

 由于数据分布的差异性,即使模型在一个数据集上表现良好,在其他数据集不一定表现好,此外也需要验证集来测试性能,为了评估模型的泛化能力和避免过拟合

- 训练集（Train Set）：训练集是用于训练机器学习模型的样本数据集合。模型通过训练集学习数据的模式、特征和规律，以建立预测或分类模型。训练集通常是最大的数据集部分，用于模型的参数估计和调整。

- 验证集（Validation Set）：验证集是用于模型选择和调优的样本数据集合。在训练过程中，使用验证集评估模型的性能，并进行超参数调整、模型选择和特征选择。验证集的目的是帮助选择最佳的模型配置，以提高模型的泛化能力。

- 测试集（Test Set）：测试集是用于最终评估和报告模型性能的样本数据集合。测试集是模型未见过的数据，用于检验模型对新数据的泛化能力。通过使用独立于训练和验证过程的测试集，可以更客观地评估模型在实际应用中的表现

  ## 3.什么是分类和回归？两者的联系和区别分别是什么？可结合图详细说明。

 分类（Classification）：旨在根据给定的输入数据，将其分配到事先定义好的不同类别中。分类问题的目标是建立一个分类器，能够根据输入数据的特征将其正确地分类到已知的类别中。例如，根据患者的症状和检测结果，将其分为患有某种疾病和健康两类。

回归（Regression）：旨在预测连续数值型输出变量，而不是离散的类别。回归问题的目标是建立一个回归模型，能够根据输入数据的特征预测出相应的输出值。例如，根据房屋的面积、卧室数量和位置等特征，预测房屋的售价。

简单来说,分类是离散的,回归是连续的.

<img src="https://i.imgur.com/UHhpvs6.png" alt="image-20231007170649917" style="zoom:50%;" />

如上图,红线可以表示一个回归的解,表示y与x的关系.

![image-20231007170833543](https://i.imgur.com/CTaXWCe.png)

而上图可以表示一个分类问题,将若干个点进行分类.

## 4.为什么对于一个神经元![img](https://i.imgur.com/oAUSCzI.jpg)，并使用梯度下降优化参数w时，如果输入x恒大于0，其收敛速度会比零均值化的输入更慢？

 这与计算梯度与激活函数有关,如果x恒大于0,那么计算反向传播梯度时,容易出现累积梯度爆炸或者消失的问题,如果进行零均值化,数值更加稳定,而且激活函数一般在0附近梯度较大,也不容易出现梯度消失的问题.

## 5.什么是梯度消失和梯度爆炸？详细说明梯度消失问题是否可以通过增加学习率来缓解？

- 梯度消失（Gradient Vanishing）：梯度消失指的是在深度神经网络中，当反向传播算法计算梯度时，梯度值逐渐变小并趋近于零的现象。这意味着网络的较早层（靠近输入层）的权重更新非常缓慢，甚至不更新，从而导致这些层对于整个网络的训练几乎没有贡献。

- 梯度爆炸（Gradient Explosion）：梯度爆炸指的是在深度神经网络中，当反向传播算法计算梯度时，梯度值迅速增大，可能变得非常大甚至溢出。这使得权重的更新步骤变得非常大，导致网络参数发生剧烈的变化，难以稳定地进行训练。梯度爆炸通常发生在网络层数较多、参数初始化不当或者激活函数的导数值较大的情况下。

增加学习率也许能解决梯度消失的问题,因为增加学习率可以跳过local optimal这样使得优化时能达到最优点而不是陷入一个局部最优.

## 6.什么是过拟合？Dropout为什么有助于防止过拟合？请叙述批归一化(Batch Normalization)的4个步骤。

过拟合是指在机器学习中，模型在训练数据上表现良好，但在未见过的测试数据上表现较差的现象。当模型过度适应训练数据的噪声和细节时，会导致过拟合。过拟合通常发生在模型的复杂度高、训练数据较少或者训练过程过长的情况下。

Dropout 是一种在神经网络中广泛使用的正则化技术，有助于防止过拟合。Dropout 在训练过程中随机地丢弃一部分神经元的输出，使得网络不能依赖于特定的神经元，从而减少了神经元之间的耦合。这样可以强制网络学习更加鲁棒和泛化性能更好的特征。Dropout 还有类似于集成学习的效果(类似于多模型ensemble)，通过在每个训练迭代中随机丢弃不同的神经元，可以获得多个不同的子模型，最终将它们的预测结果平均或者进行投票来得到最终的预测结果。

批归一化（Batch Normalization）的四个步骤：

1. 计算批次的均值和方差。对于一个批次中的每个特征，在训练过程中计算其均值和方差。
2. 归一化。使用计算得到的均值和方差来对批次中的每个特征进行归一化处理，使其均值为0，方差为1。
3. 缩放和平移。引入两个可学习的参数（缩放因子和偏移量），对归一化后的特征进行线性变换，以恢复其表示能力。
4. 输出。将缩放和平移后的特征作为批归一化的最终输出。

## 7.梯度下降优化算法SGD with momentum中，参数更新时通过积累之前的动量来加速当前的梯度，其中一阶动量使用指数加权平均来计算： ![img](https://i.imgur.com/CPVYU8c.jpg) ![img](https://i.imgur.com/4yw5L42.jpg)

一阶动量是各个时刻梯度方向的指数加权平均数，约等于最近![img](https://i.imgur.com/wfrgRek.jpg)个时刻的梯度向量和的平均值。请简要推导为什么是最近![img](https://i.imgur.com/CRTxSYv.jpg)个时刻？SGD with momentum与标准SGD相比，它的优点和原理是什么？（可结合图论述)

一阶动量的计算公式为：
$$
m_t = β * m_{t-1} + (1 - β) * g_t
$$
其中，m_t 表示第 t 个时刻的一阶动量，g_t 表示第 t 个时刻的梯度向量，β 是一个介于 0 到 1 之间的参数，用于控制历史梯度的权重衰减程度。
$$
m_t = β * (β * m_{t-2} + (1 - β) * g_{t-1}) + (1 - β) * g_t
= β^2 * m_{t-2} + β * (1 - β) * g_{t-1} + (1 - β) * g_t
$$

$$
m_t = β^2 * (β * m_{t-3} + (1 - β) * g_{t-2}) + β * (1 - β) * g_{t-1} + (1 - β) * g_t
= β^3 * m_{t-3} + β^2 * (1 - β) * g_{t-2} + β * (1 - β) * g_{t-1} + (1 - β) * g_t
$$

以此类推，展开到第 k 个时刻，可以得到：
$$
m_t = β^k * m_{t-k} + β^{k-1} * (1 - β) * g_{t-k+1} + β^{k-2} * (1 - β) * g_{t-k+2} + ... + (1 - β) * g_t
$$
由于 β 是介于 0 到 1 之间的参数，当 k 趋近于无穷大时，β^k 会趋近于 0。也就是说，历史梯度的权重会随时间指数级地衰减。

由级数展开可知,β从0到k的幂次和为1/(1-β).

因此，一阶动量的计算约等于最近 1/(1-β) 个时刻的梯度向量和的平均值。随着时间的推移，较早的梯度对于当前一阶动量的贡献会越来越小，而最近的梯度对于当前一阶动量的贡献会更加显著。这样选择最近几个时刻的梯度向量可以更好地反映当前的梯度方向，并且保持一定的历史信息。

 带动量的梯度下降能更容易地摆脱local optimal,动量可以平滑梯度的更新过程，使得梯度更新更加稳定。通过一阶动量的累积，动量项可以减少梯度方向的抖动，使得梯度更新更加平滑

<img src="https://i.imgur.com/DdlAtuD.png" alt="image-20231007173533854" style="zoom:50%;" />

如图,如果是普通的SGD,可能会卡在局部最优点B,而带动量的SGD能通过累计之前的梯度摆脱这个局部最优点到达总体最优D.

## 8.梯度的定义：某一函数沿着某点处的方向导数可以以最快速度达到极大值，该方向导数定义为该函数的梯![img](https://i.imgur.com/fodOobL.jpg) 其中![img](https://i.imgur.com/Jgteowz.jpg)是自变量，![img](https://i.imgur.com/2BNzyIj.jpg)是关于![img](https://i.imgur.com/SBtJpH8.jpg)的函数，![img](https://i.imgur.com/gms3x45.jpg)表示梯度，推导以下公式：![img](https://i.imgur.com/L4SDA7m.jpg) 

利用泰勒展开
$$
f(\theta)\approx f(\theta_0)+(\theta-\theta_0)\cdot\nabla f(\theta_0)
$$

$$
\theta-\theta_0=\eta v
$$


$\theta-\theta_0$不能太大,太大的话,线性近似就不够准确,一阶泰勒近似也不成立了。替换之后,$f(\theta)$的表达式为,
$$
f(\theta)\approx f(\theta_0)+\eta v\cdot\nabla f(\theta_0)
$$

局部下降的目的是希望每次$\theta$更新,都能让函数值$f(\theta)$变小。也就是说,上式中,我们希望$f(\theta)<f(\theta_0)$。则有:

$$
f(\theta)-f(\theta_0)\approx\eta v\cdot\nabla f(\theta_0)<0
$$
∇f(θ)是当前位置的梯度方向，v表示下一步前进的单位向量,η表示前进大小

当v与∇f(θ0))互为反向，即v为当前梯度方向的负方向的时候，能让v⋅∇f(θ0)最大程度地小，也就保证了v的方向是局部下降最快的方向
$$
\vec{v}=-\frac{\nabla f(\theta_0)}{\|\nabla f(\theta_0)\|}
$$
所以有
$$
\theta=\theta_0-\eta\nabla f(\theta_0)
$$
